\section{Conclusion}
\label{sec:conclusion}

The substantial and continuing advancements of LLMs since 2022 have enabled powerful new paradigms, such as MAAI, capable of automating complex human tasks. However, as industry moves MAAI systems into production, their underlying conceptions of fairness present a pressing societal concern. Whereas bias and discrimination are well-documented in analytical AI, MAAI represents a significant research gap. This thesis experimentally addresses four research gaps identified by \citet{Allmendinger_2025_MAAI_Fairness_Unpublished}: the absence of humans, the homogeneous use of LLMs, the emergence of system dynamics, and the influence of input language. Taking a human-centric and systemic lens, this work replicates the group fairness experiments by \citet{Frohlich_Oppenheimer_1992_Book} with AI agents. These experiments are based on the philosophy in particular the original position of John Rawls~\citep{Rawls_1971_theory_justice}.

Whereas this thesis does not investigate the interaction between humans and MAAI, it takes a first step in addressing the Human Exclusion Problem. It does so by comparing the fairness judgments of human groups and an MAAI consisting solely of AI agents. This reveals significant differences in fairness judgments. Whereas both populations favor the same conception of fairness by favoring maximizing average income with a floor constraint, MAAI demonstrate substantially reduced variance, with 90.9\% selecting this principle compared to 79.4\% among human groups. This concentration indicates that MAAI replicate the most common human fairness preference while potentially suppressing minority viewpoints. The finding raises questions about the normative implications of replacing human decision making with MAAI, particularly regarding the preservation of minority viewpoints and the plurality of opinion.

Crucial to the fairness judgments of MAAI is the influence of the LLM on fairness outcomes. This thesis finds that MAAI based on American LLMs show more egalitarian preferences compared to Chinese LLMs. Whereas the LLM affecting fairness outcomes is expected, it is surprising that American LLMs, which are created in a culture that emphasizes individual responsibility and risk-taking, lead to more egalitarian results. Although the testing of this hypothesis is limited due to budget constraints, it demonstrates the influence of LLM choice on fairness outcomes. The research community is urged to further evaluate the origin and influence of LLMs in future fairness research. More research and a categorization of LLMs are needed to better inform practitioners on the normative implications of their LLM choice.

System dynamics emerge as a critical dimension of MAAI fairness. Intelligence asymmetries create substantial manipulation vulnerabilities. High-capability agents successfully reshape group consensus in 33\% of experiments, turning fairness preferences upside down. Low-capability agents fail to exert comparable influence, succeeding in only 3\% of cases. These findings suggest that prompt-level fairness interventions might be insufficient to reliably embed normative goals and point toward the necessity of architectural-level safeguards, for example, through fine-tuning the underlying LLM. Future research should compare different techniques to embed fairness in single AI agents and entire MAAI to create reliable systems. 

Language selection significantly affects fairness outcomes. The initial preference of the AI agents differs significantly between all three languages. During deliberation, Spanish configurations produce higher consensus failure rates and greater outcome variance compared to English and Mandarin configurations. This suggests that LLM capabilities vary across languages. Throughout the duration of the experiment, the fairness preference convergence across languages suggests that multi-agent interaction itself exerts homogenizing effects on preferences, though this is insufficient to overcome language-specific barriers to consensus. The research community is urged to take the role of language into account in future fairness in MAAI and further examine its influence, for example, with regard to the sensitivity towards payoff. Practitioners should be aware of the influence of input languages and may even use it as a deliberate design choice depending on their fairness objective.

This thesis presents a novel approach to investigating fairness in MAAI. It shows that MAAI exhibit distinct fairness judgments differing from humans, while being sensitive to the underlying LLM, the input languages, and manipulation. The presented findings are a stepping stone for further research which is needed in an effort to create fair MAAI systems serving humans. 


