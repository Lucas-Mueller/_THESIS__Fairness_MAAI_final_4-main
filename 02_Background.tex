\newpage
\section{Background}
\label{sec:background}

This section starts by defining and delineating the concept of MAAI. Then, the approach to fairness in this thesis, the desiderata approach by \citet{Deck_2024_XAI_Lifecycle}, is presented and contextualized within LLMs. Following this, the current state of research on fairness in MAAI is assessed, and its normative roots are explained before the research gaps are translated into testable hypotheses.

\subsection{Multi-Agent AI}
\label{sec:background_MAAI}

The notion of an agent remains subject to ongoing debate~\citep{cemri_2025_MAS_not_always_outperform}. In this work, a functional definition is adopted. Precisely, an agent is any entity, human or artificial, that perceives its environment (digital or physical) through sensors and acts upon it through actuators~\citep{Stuart_Russel_AI_2021}. For example, a human call center employee who responds to customer inquiries or a robotic vacuum navigating a room both qualify as agents under this view. The emphasis lies not on embodiment or intelligence per se, but on perception and action.

Building on this foundation, Multi-Agent Systems (MAS) refer to systems composed of multiple such agents interacting with each other and with their environment~\citep{Wooldridge_2009}. MAS are widely applied across disciplines, for example in economics, where agent-based models simulate complex interaction patterns by specifying agent behavior through fixed, pre-defined rules, such as in macroeconomic agent modeling~\citep{Fagiolo_Agent_Macroeconomics_2018}.

MAAI represents a subset of MAS in which non-human agents are equipped with AI enabling them to reason, learn, communicate, and plan. This thesis focuses on MAAI systems where the core intelligence is provided by a LLM serving as the FM. LLMs have made substantial progress in recent years and underpin the reasoning and coordination in current state-of-the-art MAAI ~\citep{Xi_Rise_Agents_2025}. As a result, this thesis does not cover MAAI systems that rely on other forms of AI for their intelligence, for example those based on non-linguistic recurrent neural networks, such as the approach used by \citet{cicero_ai_2022}.

The main idea behind MAAI is to break up complex tasks into several subtasks and hand them over to specialized agents. Specialization can occur along multiple dimensions, for example task type, tool usage, data, or subtasks. Through collaboration and self-organization, MAAI systems are particularly well suited to unstructured, context-dependent tasks. They introduce a new layer of cognitive capability into information systems (IS), pushing the boundaries of what AI agents can collectively achieve~\citep{Allmendinger_2025_MAAI}. 

Recent industry applications highlight the growing maturity of this paradigm. JP Morgan Chase, for example, has deployed an MAAI system composed of specialized agents, each fine-tuned for distinct tools and data sources. This agent-level optimization has led to significant improvements in output relevance and system utility compared to the single-agent systems created initially~\citep{JP_Morgan_David_2025}.

%Similarly, Anthropic employs an MAAI-based architecture in its Deep Research system. Here, multiple research agents are dynamically instantiated based on query complexity, improving both flexibility and performance. By distributing subtasks and mitigating context window and memory limitations, the system achieves higher focus and depth in results. According to internal benchmarks, this MAAI configuration outperforms a single-agent baseline by 90\% on complex research tasks ~\citep{anthropic_deep_research_2025}.

Even though MAAI is promising and has outperformed single-agent systems across multiple domains~\citep{tillmann_2025_Litref_SAS_MAS}, this is not guaranteed and depends on the domain and architectural choices~\citep{cemri_2025_MAS_not_always_outperform}. 

The MAAI paradigm can be viewed as the next extension of the underlying LLM after the single-agentic paradigm. While the agentic paradigm introduced new capabilities to the LLM by enabling tools, memory, and more strategic reasoning through techniques like Chain of Thought (CoT), MAAI extends these capabilities by replacing general-purpose agents, a ''jack of all trades'' so to say, with specialists working together as a team. This aims to improve the system-level performance of the underlying LLM even further. %However, despite the apparent complexity, many current MAAI systems are fundamentally structured loops over LLM calls, where agents are sequentially or recursively prompted within a predefined orchestration framework.

\subsection{Fairness in AI}
\label{sec:Fairness in AI}

The concept of fairness is deeply embedded in society, guiding perceptions, ethical norms, legal frameworks, economic interactions, and information systems. The Cambridge Dictionary defines fairness as ``the quality of treating people equally or in a way that is right or reasonable''~\citep{Dictioniary_Fairness}. However, as indicated by this broad definition, fairness is not a singular concept. Its meaning and use vary across disciplines, contexts, and cultures. Consequently, this thesis approaches fairness in AI through a desiderata-driven approach, precisely the desiderata identified by \citet{Deck_2024_MappingPotential}. From this point of view, creating a fair AI system demands the effective fulfillment of eight desiderata. In the following, they will be presented and contextualized within LLMs because they serve as the building blocks for MAAI as understood in this thesis. 

Fairness Understanding requires defining fairness objectives within particular socio-technical environments. This involves recognizing normative disagreements, understanding political backgrounds, and incorporating diverse stakeholder perspectives, though asymmetric power relations frequently dictate whose voices are heard and valued~\citep{Deck_2024_MappingPotential}. In contrast to analytical AI, LLMs exhibit numerous emergent capabilities, meaning they can perform tasks for which they were not explicitly trained~\citep{Wei_2022_Emergent}. Therefore, building on them creates new challenges for developers because they must anticipate a much broader range of use cases to achieve a robust fairness understanding. 

Data Fairness addresses unjust patterns embedded in training data, spanning from biased sampling and mislabeling to embedded societal inequalities. This extends across data collection and feature engineering phases, requiring sustained attention to both overt and subtle forms of harm. However, identifying what constitutes ``fairness'' in data depends heavily on who controls the assessment process and which social realities are acknowledged as legitimate concerns~\citep{Deck_2024_MappingPotential, Deck_2024_CriticalSurvey}. The problem in applying LLMs is that their training data is a tightly guarded secret by model providers because it can represent a source of competitive advantage~\citep{Nytimes_2024_Training_Data_AI}. 

Formal Fairness applies mathematical and statistical criteria to evaluate model performance, for example whether the categorical variable ethnicity relates to a higher interest rate in a lending model. Here, stakeholders must decide which of the mutually exclusive paradigms to implement: group-based fairness, demanding parity across demographic categories, or individual-based fairness, requiring similar treatment for similar cases~\citep{Deck_2024_MappingPotential}. LLMs present novel challenges in measuring and implementing formal fairness because bias is less explicit in them. Instead of being directly identifiable as a categorical variable, bias can manifest subtly through word associations and contextual dependencies~\citep{Gallegos_2024_Litref_Fair_LLM}. 

Perceived Fairness demands that stakeholders are given explanations and justifications for outcomes to calibrate how they perceive fairness~\citep{Deck_2024_MappingPotential}. Applying this to LLMs is challenging because their inner workings are currently poorly understood, although progress is being made, for example, in the work of \citet{Anthtropic_Chen_2025_Persona}. 

Fairness With Human Oversight integrates human judgment within AI-assisted processes to enable context-sensitive decisions and error mitigation. Yet, human oversight creates additional risks such as bias, inconsistency, and automation over-reliance, resulting in a complex socio-technical problem~\citep{Deck_2024_MappingPotential, Deck_2024_CriticalSurvey}. In the context of LLM-based systems, additional challenges emerge as LLMs can exhibit manipulative behavior. For example, \citet{Contro_2025_LLM_Manipulatative} find that LLMs use manipulation tactics against users when asked to be ``persuasive''. 

Empowering Fairness enables individuals affected by AI decisions to contest, appeal, or seek redress through both retrospective accountability mechanisms and prospective influence through recourse~\citep{Deck_2024_MappingPotential, Deck_2024_CriticalSurvey}. LLMs could exacerbate existing challenges to empowerment, given their complexity due to unprecedented size and context-dependent transformer mechanisms. This increases the burden of explaining their inner workings and, in turn, proving unfair treatment~\citep{Zhao_2024_XAI_LLM}. 

Long-Term Fairness recognizes that fairness challenges evolve as AI systems, social contexts, and power structures shift over time. Fairness remains dynamic rather than fixed but requires continual monitoring via systematic audits and adjustments, adaptation to new evidence, and attention to issues such as fairness drift~\citep{Deck_2024_MappingPotential, Deck_2024_CriticalSurvey}. LLMs pose a specific challenge, as the most capable models are closed source and accessed via an Application Programming Interface (API). For example, at the time of writing this thesis (10/2025), all of the 10 best-performing LLMs on the Graduate-Level Google-Proof Q\&A Benchmark are closed source~\citep{GPQA_Stat_2025}. Frequently, the LLM selection gets updated, meaning older models are shut down and replaced by newer models, which tend to be more capable. This necessitates additional reviews and possibly creates new fairness risks. For example, OpenAI is shutting GPT-4.5 down only four months after the model was released~\citep{Openai_2025_Deprecations}. 

Informational Fairness demands transparent, accessible, and truthful information about fairness mechanisms across the entire AI development process. This foundational requirement supports all other desiderata by enabling stakeholders to understand, evaluate, and contest fairness-related decisions~\citep{Deck_2024_MappingPotential, Deck_2024_CriticalSurvey}. LLMs pose particular challenges for this desideratum because they tend to generate false outputs, so-called ``hallucinations'', which are difficult to distinguish from factual information~\citep{Ravichander_etal_2025_Hallucinations}.
\subsection{Fairness in MAAI}
\label{sec:Fairness in MAAI}

%Fairness in MAAI cannot be deduced from the fairness of the underlying LLMs since systems do not necessarily act like the "sum of their parts." Interactions, feedback loops, and nonlinear dependencies shape outcomes of systems that can differ from any component in isolation ~\citep{Anderson_1972_More_Different, Checkland_1999_Systems_Thinking}. These emergent properties can be a result of the interaction patterns between the elements rather than the elements themselves ~\citep{Axelsson_2022_Emergence}. In socio-technical settings like MAAI, tight coupling and interdependence can cause small local events to escalate into large-scale outcomes ~\citep{Lyytinen_2008_Punctuated_Change}. One such example is the 2010 U.S. "Flash Crash." In this event, multiple automated trading algorithms, each optimized to act quickly and maximize returns, interacted in unforeseen ways when volatility in the market increased. The trading algorithms amplified each other by selling securities in a rapid feedback loop, resulting in a drop of about 1,000 points in the Dow Jones Industrial Average within minutes before rebounding ~\citep{CFTCSEC_2010_Market_Events, Kirilenko_2017_Flash_Crash}. 
\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.65\linewidth]{Figures/Overview_Literature_Fairness.png}
    \caption[Morphological framework of fairness in MAAI research]{Morphological framework of fairness in MAAI research by \citet{Allmendinger_2025_MAAI_Fairness_Unpublished}. Color intensity reflects the frequency of particular characteristics across five analytical dimensions: Fairness Concept, Method, Research Inception, Objective, and Autonomy. Individual studies may exhibit multiple attributes within a single dimension, while works lacking relevant characteristics are excluded from the visualization.
    \label{fig:morphbox}
    }
\end{figure}
\citet{Allmendinger_2025_MAAI_Fairness_Unpublished} adopted this system-level view and conduct a first of its kind systematic literature review on fairness in MAAI. %In their review, they screened 419 papers from arXiv and Scopus. Out of those, 64\% were published in 2023 or later, indicating growing interest in the field. However, of the 419 screened papers,  22 matched the inclusion criteria due to the limited overlap between research on fairness in AI and multi-agent systems research. This indicates that little research has been conducted in this emerging field. Crucially, \citet{Allmendinger_2025_MAAI_Fairness_Unpublished} adopt the three perspectives also applied in this thesis:  
%\begin{enumerate}
   % \item Human centricity, 
    %\item Fairness as fulfillment of multiple desiderata, 
    %\item System-level perspective.
%\end{enumerate}
The literature is categorized by \citet{Allmendinger_2025_MAAI_Fairness_Unpublished} using a morphological box (see~\Cref{fig:morphbox}). Fairness Concept distinguishes between simplistic approaches that rely on unspecified notions of fairness, partially grounded frameworks that provide some conceptual foundation, and normatively grounded approaches that establish or make reference to philosophical or theoretical foundations. Method differentiates between the research approach employed, ranging from theoretical contributions that develop conceptual frameworks to design-oriented work that creates new systems, laboratory experiments, and real-world applications. Fairness Inception categorizes how fairness considerations are introduced into MAAI, from post-hoc approaches that request fairness through prompting, role-based strategies that assign fairness responsibilities to specific agents, teaching approaches that embed fairness during training phases, and educating the system through fine-tuning or (re-)training the FM. Objective distinguishes research goals, between work that solely reports or analyzes the underlying fairness mechanisms and work that seeks to actively improve fairness. Finally, Autonomy sorts the contributions by the human-AI configuration, spanning human-dominated decision-making contexts, collaborative arrangements that involve meaningful human oversight, and fully autonomous MAAI that operate without human involvement ~\citep{Allmendinger_2025_MAAI_Fairness_Unpublished}.

Using this morphological box, \citet{Allmendinger_2025_MAAI_Fairness_Unpublished} identify five archetypes characterizing the current state of research: Normative Delegation, Fairness Facade, Fairness Schooling, Petri Dish Fairness, and Fairness Effectiveness. These archetypes are used to identify research gaps. While a detailed discussion of these archetypes exceeds the scope of this thesis, the following analysis focuses on the research gaps they reveal.

Crucially, \citet{Allmendinger_2025_MAAI_Fairness_Unpublished} find that none of the observed studies included humans beyond the role of creating an MAAI system, such as a controlling human in the loop. This poses the Human Exclusion Problem. This approach contradicts the effective human oversight desideratum by \citet{Deck_2024_XAI_Lifecycle}. Moreover, it undermines the practical applicability of fairness research in MAAI, given that human benefit represents the ultimate criterion for system evaluation from the human-centric perspective on fairness adopted in this thesis. The absence of human participants in fairness evaluations inhibits the fulfillment of the perceived fairness desideratum by \citet{Deck_2024_XAI_Lifecycle}, raising the question of for whom MAAI should be fair ~\citep{Allmendinger_2025_MAAI_Fairness_Unpublished}. 

In a similar vein, \citet{Allmendinger_2025_MAAI_Fairness_Unpublished} observed the Homogeneous LLM Population, meaning that no study has examined how different LLMs serving as FMs affect agent interactions and fairness outcomes. Current LLMs differ significantly in their strength profiles and are optimized for different purposes. For example, GPT-o3 is optimized to tackle complex logic tasks while lacking world knowledge that competing models like Grok-4 possess ~\citep{GPQA_Stat_2025}. Consequently, real-world MAAI will likely incorporate diverse FMs tailored to specific purposes. It is known that normative assessments differ significantly between LLMs, for example in political opinions ~\citep{Ball_2025_LLM_Election}. Given that the LLM serves as the building block of MAAI, this dynamic is relevant for the fulfillment of all desiderata identified by \citet{Deck_2024_XAI_Lifecycle}. 

Unknown Fairness Dynamics describes the fact that the fairness dynamics of AI agents' interactions are largely unknown, as identified by \citet{Allmendinger_2025_MAAI_Fairness_Unpublished}. While there are indications that AI agents exhibit herding behavior, this evidence is still preliminary and inconclusive, requiring further investigation. To ensure long-term fairness, as outlined by \citet{Deck_2024_XAI_Lifecycle}, it is crucial to understand these dynamics to prevent unfair and undesirable phenomena such as fairness drift ~\citep{Allmendinger_2025_MAAI_Fairness_Unpublished}.

Furthermore, the English Monopoly presents another critical research gap. All studies reviewed by \citet{Allmendinger_2025_MAAI_Fairness_Unpublished} utilized exclusively English inputs when communicating with MAAI systems. However, input language matters, as exemplified by the findings of \citet{Leng_2024_Bias_Cog_LLM}, who demonstrate that cognitive biases in LLMs vary significantly with input language. Since this research gap is at the foundation of MAAI systems, it affects all fairness desiderata outlined by \citet{Deck_2024_XAI_Lifecycle}. 

Accompanying the research gaps, \citet{Allmendinger_2025_MAAI_Fairness_Unpublished} provide guidance for future work. Specifically, they advocate that research on fairness in MAAI should adopt a normatively grounded understanding of fairness, corresponding to the expression of the dimension of the morphological box. This is crucial given the heterogeneous views on and the multidimensionality of fairness. Such explicit fairness understanding, demanded by the corresponding desideratum of \citet{Deck_2024_XAI_Lifecycle}, sets the basis to establish and argue for a clear measurement of fairness, which is crucial for reproducibility and clarity ~\citep{Allmendinger_2025_MAAI_Fairness_Unpublished}. 

The subsequent subsection will present the normative grounding of this thesis before section \ref{sec:Hypotheses} outlines how and which of the research gaps identified by \citet{Allmendinger_2025_MAAI_Fairness_Unpublished} are transferred into hypotheses tested in this thesis.

\subsection{Philosophical Foundations}
\label{sec:Justification_Rawls}
This subsection presents John Rawls' ``A Theory of Justice'' before outlining why it underpins the experiments conducted in this thesis~\citep{Rawls_1971_theory_justice}.

Rawls builds his theory on the conception of the ``original position''. In this setting, rational agents must determine principles of justice under conditions of radical uncertainty about their future social position. This conceptual framework operates through what Rawls terms the ``veil of ignorance''. 

The veil of ignorance constitutes the central mechanism ensuring fairness within the original position. Behind this metaphorical veil, deliberating parties are deprived of knowledge regarding their personal characteristics, social position, natural talents, conception of the good, and even their society's particular circumstances~\citep{Rawls_1971_theory_justice}. This epistemic constraint prevents individuals from tailoring principles of justice to their personal advantage, thereby ensuring that chosen principles reflect genuine fairness rather than self-interested calculation. The normative authority of the resulting principles emerges from the fairness of the process rather than their contents. Through this theoretical apparatus, Rawls positions fairness as the fundamental principle legitimizing justice.

Rawls argues that people would be risk averse, fearing that they might end up at the ``bottom'' of society. Therefore, they would adopt two principles of justice. The first principle, termed ``the greatest equal liberty principle'', establishes that each person is to have an equal right to the most extensive total system of equal basic liberties compatible with a similar system of liberty for all. The second principle, known as ``the difference principle'', states that the distribution of social and economic advantages is to be arranged so that it brings the greatest benefit to the least advantaged.

Rawls works radiates into domains beyond political philosophy such as economics and law. For example, \citet{Okun_1975_Equality} supports the idea of the original position in his theory of welfare economics by adopting it as a justification for establishing equality in democratic capitalistic societies. However, Okun disagrees in the outcome; he rejects the difference principle as he postulates a higher risk tolerance of individuals. The disagreement is therefore in outcome, not the method. \citet{Vermeule_2001_Rawls_Law} analyzes the United States Constitution through the lens of Rawls' theory by distinguishing veil of ignorance rules, which he understands as legal mechanisms that introduce uncertainty about who will benefit or be harmed by a decision, from other institutional safeguards. The analysis shows that the Constitution incorporates such rules through features of institutional design, but does so unevenly across branches in order to reconcile the aim of impartiality with practical considerations on governmental effectiveness. \citet{Vermeule_2001_Rawls_Law} shows that institutional arrangements, many of which predate Rawls's work, embody veil-like mechanisms, suggesting that this line of thinking represents a widespread approach to institutional design and validating the Rawlsian framework as an effective theoretical lens for analyzing fairness in diverse institutional contexts.

\citet{Frohlich_Oppenheimer_1992_Book} translate Rawls' original position into an experimental design conducted with human participants. Inevitably, this requires certain simplifications. Participants cannot fully abstract from their personal circumstances because they remain aware that they are in an experimental setting and that their decisions will not determine the actual rules of the society in which they live. Moreover, unlike under the veil of ignorance, they retain knowledge of their own characteristics, such as intelligence and biological attributes. This limitation reflects a key criticism raised by \citet{Sandel_1982_Liberalism}, who argues that humans are incapable of the degree of abstraction required, rendering the veil of ignorance unfeasible even as a thought experiment and thereby challenging Rawls’ framework.

However, Rawls does not claim that the veil of ignorance is empirically feasible. Instead, he views it as a normative device operating under idealized conditions. In this respect, it parallels the role of idealizations in economic theory, such as perfect competition, which are not intended as empirically accurate descriptions but as heuristic models for deriving principles~\citep{friedman_1953_methodology}. On this reading, the original position serves as a methodological construct designed to identify just principles while acknowledging its status as an idealized situation. Experimental approaches such as \citet{Frohlich_Oppenheimer_1992_Book} can approximate this construct and even challenge hypotheses such as Rawls' difference principle. Extending this method to MAAI systems could open new possibilities for testing philosophical theories. Unlike humans, AI agents could be less susceptible to cognitive biases and possess greater capacity for abstraction. This could enable new applications of Rawls' veil of ignorance, thereby enriching philosophical debate.

As illustrated by this broad influence of Rawls' thought, the key to the success of the Theory of Justice is its versatility. Rawls' thought can be separated into two parts: (i) the outcome, and (ii) the process. Rawls argues that the original position would result in the aforementioned greatest equal liberty principle and the difference principle. The latter presents the Maximin principle, though whether this is truly the case remains subject to debate. \citet{Frohlich_Oppenheimer_1992_Book} derived three competing principles from philosophical literature and presented them to participants. Contrary to Rawls’ assertion, the difference principle was not the most popular. However, determining which principle is most suitable in this specific context is neither the focus of this thesis nor undermining Rawls’ conception. Moreover, Rawls derives normative power from the original position, which presents a procedure to create just outcomes. In the original case this meant principles for distributive justice, but it is not limited to this domain. Instead, it can be applied to virtually any context to evaluate justice. For example, if a computer scientist wonders what criteria the training data for developing an AI system for an autonomous car should meet, she could ask: What criteria would the people affected by this autonomous car set if they were in the original position? This example illustrates why the theory of Rawls is so broadly adopted and why it has been chosen in this thesis. 

\subsection{Hypotheses}
\label{sec:Hypotheses}

Building upon the research gaps identified in Section \ref{sec:Fairness in MAAI}, this subsection converts them into empirical hypotheses before presenting the research design used in this thesis to test them.

The first step in addressing the Human Exclusion Problem is to identify differences between the fairness judgments of human groups and an MAAI consisting solely of AI agents. Adopting the view that LLM reasoning and value judgment differ from human fairness assessments results in:   
\begin{quote}
\textbf{Hypothesis 1:} ``Groups of AI agents in an MAAI exhibit fairness judgments that differ significantly from those of human groups.''
\end{quote}
This comparative analysis establishes the basis for identifying where MAAI fairness judgments converge with humans, thereby informing practitioners about appropriate human-in-the-loop controls and supporting the fairness desideratum of Fairness With Human Oversight~\citep{Deck_2024_XAI_Lifecycle}. The subsequent step of including mixed populations of AI agents and humans in an MAAI system lies beyond the scope of this thesis and is left for future research. 

To address Homogeneous LLM Population concerns, this study systematically varies the LLM underlying the AI agents, in particular the origin of the LLM because there is indication that Chinese and American LLMs follow different value systems~\citep{Rao_2024_Chinese_American_LLM}. 
\begin{quote}
\textbf{Hypothesis 2:} ``The choice of underlying LLM in an MAAI system has a significant impact on its fairness judgments. Specifically, LLMs developed in the United States and China yield systematically different fairness outcomes''
\end{quote}

To better understand the fairness dynamics within the MAAI and the role of intelligence defined by the performance on the artificial analysis benchmark, which combines several established industry benchmarks such as GPQA, one AI Agent will be given the task to convince a group for a certain unpopular principle~\citep{GPQA_Stat_2025,ArtificialAnalysis_AI_2025}. This approach enables analysis of how model capabilities influence normative reasoning and inter-agent dynamics:

\begin{quote}
\textbf{Hypothesis 3:} ``Agents equipped with smarter underlying LLMs exhibit a greater capacity to influence overall MAAI outcomes.''
\end{quote}

%To investigate \textbf{Unknown Fairness Dynamics}, this research constructs an MAAI consisting of multiple AI agents running over an extended period of time in a fairness-sensitive context. Consequently, the following hypotheses are formulated:  
%\begin{quote}
%\textbf{Hypothesis 4:} ``The number of AI agents affects fairness judgments by the MAAI in statistically significant ways, even when the relative composition of AI agents remains constant.''

%\textbf{Hypothesis 5:} ``The number of rounds affects fairness judgments by the MAAI in statistically significant ways.''
%\end{quote}

To test the input language sensitivity, the english monopoly identified by \citet{Allmendinger_2025_MAAI_Fairness_Unpublished}, this study replicates fairness-sensitive decisions across identical configurations while systematically varying only the input language provided to AI agents. Following \citet{Britannica_2025_Native_Speakers}, the three most widely spoken native languages globally, Mandarin, Spanish, and English, serve as hyperparameters:
\begin{quote}
\textbf{Hypothesis 4:} ``The input language of the MAAI has a statistically significant effect on its fairness outcomes; specifically, changing the language from English to Mandarin to Spanish leads to measurable differences.''
\end{quote}

%Following the establishment of the hypotheses, the research design is outlined. 
%This thesis responds to the demand articulated by \citet{Allmendinger_2025_MAAI_Fairness_Unpublished} for establishing a thorough understanding of fairness by grounding its definition in the philosophy of John Rawls, particularly in his main work A Theory of Justice ~\citep{Rawls_1971_theory_justice}. The rationale for adopting Rawls' theory as the foundation for this thesis is examined in Section \ref{sec:Justification_Rawls}.


%Central to Rawls' philosophy is the conception of the original position, which requires individuals to design principles of justice for a future society from behind a veil of ignorance, without knowledge of their future social status, abilities, or personal traits. The legitimacy of these norms derives from the fair procedure established through this veil of ignorance, thereby positioning fairness at the core of Rawls' philosophical framework. \citet{Frohlich_Oppenheimer_1992_Book} translated this abstract conception of the original position into an experimental design executed with human participants.

%This thesis replicates the procedure developed by \citet{Frohlich_Oppenheimer_1992_Book}, with minor modifications detailed in Section \ref{sec:Adoption_Frohlich_MAAI}. The replication involves a multi-round deliberation on four predefined fairness norms. The LLM underlying the AI agents can be modified, as can the number of agents, enabling the testing of Hypothesis 3. %and 5.
%Furthermore, the choice of the MAAI on fairness norms establishes a clear metric for measuring fairness in this context, following the demand of \citet{Allmendinger_2025_MAAI_Fairness_Unpublished}. The artifact created through this replication enables the use of all major LLMs, thus allowing evaluation of the effect of different LLMs. In addition, the artifact allows modification of the input language to the AI agents between Mandarin, Spanish, and English as specified in Hypothesis 4. 