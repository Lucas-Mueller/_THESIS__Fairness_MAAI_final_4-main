\section{Introduction}
\label{sec:introduction}

Generative Artificial Intelligence (GenAI) has achieved substantial progress in recent years, with large language models (LLMs) approaching or surpassing human performance across domains~\citep{Masley_etal_2024_ai-index}. Multi-Agent Artificial Intelligence (MAAI) has emerged as a paradigm that further enhances performance and utility through coordinated systems of independent or partially independent artificial intelligence (AI) agents~\citep{Li_2024_MAAI_Paradigm, Allmendinger_2025_MAAI}. Fairness challenges in analytical AI are extensively documented~\citep{DeArteaga_2022_Algorithmic_Fairness}. 



Research examining fairness in GenAI predominantly focuses on the performance of individual foundation models (FMs)~\citep{Truong_2025_Fairness_Genai_Infancy}, while research on fairness in MAAI systems remains critically underexplored~\citep{Allmendinger_2025_MAAI_Fairness_Unpublished}.

This thesis experimentally tests four hypotheses derived from research gaps identified by \citet{Allmendinger_2025_MAAI_Fairness_Unpublished}. The research investigates whether MAAI and human groups differ in their fairness judgments, whether the choice of underlying LLM affects fairness outcomes, whether MAAI can be systematically influenced by individual agents, and whether input language shapes fairness decisions. To address these questions, this study replicates the experimental design of \citet{Frohlich_Oppenheimer_1992_Book} in an MAAI. The implementation is made available on Github~\citep{Mueller_2025_Rawls_Github_Repo}. 

The normative lens for this thesis is Rawls' theory of justice~\citep{Rawls_1971_theory_justice}. Central to his conception is the ``original position,'' which requires individuals to design principles of justice from behind a ``veil of ignorance'' without knowledge of their future social position, abilities, or circumstances. This ignorance ensures impartiality, thereby creating a fair procedure that legitimizes the resulting principles. Rawls' philosophy was selected because it represents a cornerstone of liberal thought with influence extending beyond philosophical debate into economics~\citep{Guizzo_2023_Rawls_Econ} and law~\citep{Vermeule_2001_Rawls_Law}. Moreover, the original position can serve as a meta-framework for evaluating fairness across domains, making it particularly suitable for examining MAAI systems that operate in diverse contexts.

\citet{Frohlich_Oppenheimer_1992_Book} translated Rawls' philosophical framework into controlled laboratory conditions with human participants. They asked participants to select one of four distributive justice principles that determines income levels in a hypothetical society. Participants did not know which income class they would be assigned to, simulating Rawls' veil of ignorance. The four principles each represent a distinct conception of fairness and are rooted in different schools of philosophical thought~\citep{Frohlich_Oppenheimer_1992_Book}. The experiment comprised two phases. In the individual phase, participants ranked the principles and engaged with them through simulated economic distributions that provided monetary payoffs based on their chosen principle. In the group phase, five participants deliberated to reach consensus on one principle, which then determined their final payoffs.

This work shows that MAAI can exhibit statistically significant differences from human groups in their fairness judgments. Both populations favor the same principle, which maximizes overall income while ensuring a certain level of income for the worst-off, but MAAI demonstrate substantially higher concentration on the same principle: 29 of 33 MAAI select it compared to 23 of 34 human groups. The choice of underlying LLM substantially affects fairness outcomes. MAAI powered by American LLMs more often select the principle aligned with Rawlsian norms that prioritize the worst-off, whereas Chinese LLMs put greater emphasis on maximizing aggregate income without constraining inequality or guaranteeing minimum incomes. Strategic manipulation by individual agents succeeds when these agents possess high intelligence. A manipulator agent instructed to advocate for the least popular principle succeeds in 11 of 33 cases when powered by a capable LLM (Gemini 2.5 pro) but only once in 33 attempts with a less capable model (Gemini 2.0 flash-lite). Input language demonstrates influence on fairness choices, with Spanish configurations producing more diverse outcomes than English or Mandarin configurations.

This thesis presents a first of its kind systematic evaluation of fairness in MAAI. It finds that MAAI differ from humans in their fairness conceptions, raising questions regarding the normative dimension of automating workflows from human to MAAI systems. The findings show that MAAI fairness judgments are sensitive to the LLM and its origin as well as the language used, cautioning practitioners to make conscious choices while being aware of their consequences. In addition, MAAI can be influenced systematically by malicious agents, especially if they possess higher intelligence, suggesting the need to closely monitor fairness.

This thesis approaches MAAI fairness through three integrated perspectives. First, a human-centered perspective prioritizes stakeholder needs, human wellbeing, and value alignment rather than technical performance metrics~\citep{Taylor_2024_HumanCentric, Kuehl_2024_HumanCentric}. This view stands in contrast to emerging industry perspectives that attribute moral consideration to AI systems themselves~\citep{Anthropic_2025_Model_Welfare}. Second, this work adopts a desiderata-based approach to fairness, according to which fairness encompasses multiple, sometimes competing criteria spanning disciplinary boundaries~\citep{Mulligan_2019_ThisThing,Friedler_2021_ImpossibilityFairness,Deck_2024_MappingPotential}. This necessitates demonstrable approaches demanding explicit fairness criteria and systematic impact assessment~\citep{Chen_2023_FairnessTesting, Deck_2024_CriticalSurvey}. Third, this research examines fairness at the system level rather than evaluating individual LLMs in isolation~\citep{Friedrich_2024_AuditingInstructing}.

The remainder of this work is organized as follows. \Cref{sec:background} presents the theoretical background, establishing the conceptual foundations of MAAI, fairness desiderata, and Rawlsian philosophy. \Cref{sec:methodology} details the methodology, including the experimental design replicating \citet{Frohlich_Oppenheimer_1992_Book}, the technical architecture of the software artifact, and the statistical procedures. \Cref{sec:results} reports the results for each hypothesis, before \Cref{sec:discussion} discusses the findings in relation to existing literature and acknowledges limitations. \Cref{sec:conclusion} draws conclusions and outlines directions for future research.