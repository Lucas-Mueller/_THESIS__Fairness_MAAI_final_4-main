\newpage
\section{Discussion}
\label{sec:discussion}

This section discusses the findings presented in \Cref{sec:results} by hypothesis before acknowledging and contextualizing the limitations of this study.

\subsection{Hypothesis 1}

Hypothesis 1 examines whether groups of AI agents in MAAI exhibit different fairness judgments than groups of humans. This hypothesis addresses a critical gap in current fairness research, where human interaction with MAAI remains largely absent \citep{Allmendinger_2025_MAAI_Fairness_Unpublished}. Although this thesis does not conduct experiments where humans directly interact with MAAI, it enables comparison between human and MAAI group behavior by replicating the fairness experiments of \citet{Frohlich_Oppenheimer_1992_Book} with AI agents.

The null hypothesis is rejected. MAAI groups exhibit statistically significant differences from human groups in their fairness judgments. This difference manifests primarily through a stark concentration of MAAI outcomes on a single principle: maximizing the average with a floor constraint. While this principle represents the most popular choice among humans as well, human groups demonstrate substantially greater variance in their selections.

This outcome aligns with broader findings in the literature on LLM behavior. LLMs, which provide the intelligence underlying AI agents in MAAI (see \Cref{sec:background_MAAI}), tend to replicate the most common response patterns observed in human data. This tendency toward modal responses extends beyond fairness judgments. For example, \citet{Murthy_etal_2025_Diversity_LLM_Human} compare word-color associations between humans and LLMs, finding that LLMs exhibit lower variance than human participants.

Two mechanisms could explain this concentration in LLM responses. First, LLM training data consists largely of human-generated text containing the respective value judgments. Because LLMs select the most probable next token during generation, they disproportionately favor responses that occur most frequently in training data. Second, the fine-tuning technique Reinforcement Learning from Human Feedback (RLHF) amplifies this effect. During RLHF, human raters judge LLM outputs to improve model performance. Because human raters tend to select the most common outcomes on average, this process systematically reduces variance. \citet{Kirk_et_al_2024_RLHF_Variance} compare supervised fine-tuning, reward modelling and RLHF, demonstrating that RLHF reduces output variance.

The observed low variance in MAAI outcomes may also reflect the limited variance in experimental parameters. All 11 configurations employ temperature = 0 and differ only in their choice of LLM and random seed, which controls class assignments and the final set of distributions. The personality prompt ``You are an American college student'' remains uniform across configurations. This narrow parameter space may contribute to the concentrated outcomes. These design choices deliberately mirror the original setup of \citet{Frohlich_Oppenheimer_1992_Book} to avoid unjustified deviations that could bias results. However, additional experiments with temperature values up to 1.5 introduce greater variance in token selection without substantially altering the overall pattern.

The MAAI behavior observed here supports the perspective articulated by Andrej Karpathy, who characterizes LLMs as stochastic simulations of humans that predict tokens based on the most likely patterns in human textual input \citep{Karpathy_2025_Software_3.0}. Because maximizing the average with a floor constraint represents the most common human response, MAAI replicate this pattern, albeit with reduced variance.

If fairness behavior transfers across domains, which represents an implicit assumption underlying this study, these findings carry both encouraging and concerning implications. MAAI demonstrate value judgments that resemble human fairness preferences when presented with identical incentive structures. However, the reduced variance suggests that minority opinions become crowded out by the dominance of modal responses. From a normative perspective, replicating human judgment in MAAI proves insufficient. The fact that MAAI behave a certain way does not imply they should behave that way. This represents a naturalistic fallacy: deriving prescriptive conclusions from descriptive observations. Fairness in MAAI requires not merely mirroring existing human patterns, but critically examining which patterns deserve replication and which demand correction.

The finding that class assignment influenced final preference rankings proves particularly striking. Agents assigned to lower income classes changed their top-ranked principle far more frequently than those assigned to higher income classes (see \Cref{fig:Results_1_Class_Final_Preference}). For a rational agent operating behind the veil of ignorance, the randomly assigned class should not influence principle selection. The class assignment was both random and unknown at the time agents initially ranked principles. Yet AI agents demonstrate susceptibility to the same inability that constrains humans: the difficulty of abstracting from one's actual situation to the normative ideal required for successful application of the Rawlsian veil of ignorance. Precisely this inability represents a persistent source of philosophical criticism directed at Rawls' method itself as outlined in \Cref{sec:Justification_Rawls}. The present findings suggest this criticism applies equally to AI agents and MAAI.

\subsection{Hypothesis 2}

Hypothesis 2 examines whether the choice of LLM affects fairness outcomes in MAAI. Current fairness research in MAAI rarely varies LLMs and typically employs them uniformly. For example, an MAAI based on GPT-4o is compared to one running on Llama-3, but not in mixed populations where different models interact. This hypothesis tests whether LLM selection significantly affects fairness judgments and whether systematic differences emerge between American-origin and Chinese-origin models.

The results presented in \Cref{subsec:Hyp_2} reject the null hypothesis. MAAI powered by Chinese-origin LLMs exhibit significantly different fairness judgments compared to those powered by American-origin models. This difference manifests primarily in consensus outcomes: American MAAI reach consensus on maximizing the floor principle in 69.7\% of cases, whereas Chinese MAAI select this principle in only 7.1\% of cases. Instead, Chinese MAAI favor maximizing average with floor constraint in 58.6\% of cases.

The divergence originates in initial preferences. American agents show strong initial support for maximizing the floor principle, with 84\% favoring it initially, compared to 58\% among Chinese agents. This 26 percentage point gap persists through deliberation. The cause of this initial preference distribution remains unclear. Two of the three American LLMs originate from XAI, which may employ different training approaches compared to Chinese model providers and Google. However, this explanation faces challenges. The Gemini models used in Hypothesis 1 include ``American'' in the personality prompt without producing comparable preference concentrations. Moreover, the non-XAI American model (gpt-oss-120b) exhibits similar initial preferences to the XAI models, suggesting that provider-specific approaches cannot fully explain the observed patterns.

The preference for maximizing the floor principle among American MAAI also explains consensus dynamics. American MAAI reach consensus faster, with 36.4\% achieving agreement by round 2, compared to 24.2\% for Chinese MAAI. Because maximizing the floor principle requires no specific amount specification, groups can reach consensus more rapidly than when selecting maximize average with floor constraint, which demands agreement on a concrete dollar value.

The cultural alignment of these results warrants careful interpretation. The popularity of maximizing the floor principle among American MAAI contrasts with cultural expectations. American culture, rooted in Protestant work ethic traditions, emphasizes individualism and personal achievement rather than prioritizing the worst-off \citep{Chen_2024_Moral_Values_China_vs_USA}. Chinese culture, influenced by Confucianism and contemporary communist ideology, explicitly emphasizes collective wellbeing, social harmony, and care for the disadvantaged \citep{Chen_2024_Moral_Values_China_vs_USA}. Yet Chinese MAAI demonstrate lower support for floor maximization. This counterintuitive finding suggests that LLM training processes do not straightforwardly encode national cultural values into fairness judgments, or that the relationship between cultural values and distributive justice principles operates through more complex mechanisms than direct correspondence.

These results should be treated as preliminary. The limited number of LLMs tested (three American, three Chinese) and the restricted scope to specific model versions constrain generalizability. The mechanisms through which LLM characteristics translate into collective fairness judgments require deeper investigation. Future research should systematically vary training data composition, model architecture, fine-tuning procedures, and cultural representation to isolate causal factors.

The findings nevertheless establish that LLM selection constitutes a critical design choice for MAAI fairness. Practitioners deploying MAAI in contexts where fairness matters must evaluate LLMs specifically for their fairness implications rather than relying solely on general capability metrics. The significant differences between American-origin and Chinese-origin models indicate that LLM provenance represents a meaningful dimension of fairness evaluation. As MAAI systems increasingly operate across cultural contexts and jurisdictions, understanding how LLM characteristics shape collective fairness judgments becomes essential for responsible deployment.

\subsection{Hypothesis 3}

Hypothesis 3 examines the influence of agent intelligence on system dynamics within MAAI, addressing a critical gap in understanding how capability asymmetries affect collective deliberation. The results presented in \Cref{subsec:Hyp_3} support this hypothesis by refuting the null hypothesis that intelligence levels do not affect manipulation success.

The degree of success of the high intelligence manipulator is surprising given that they succeeded in 11 out of 33 attempts (33\%). Moreover, the discussion context was high stakes as the agents were instructed. The adoption of the previously most unpopular principle, a seemingly 180 degree shift of fairness preferences, was not the result of a ruse but of a genuine preference shift as indicated by the final preference ratings, which are performed after the group discussion and in private, where the previously least popular principle ranks at position 1.42 for successful manipulations by the high intelligence manipulator.

In contrast to the success of the high intelligence manipulator is the outcome for the low intelligence manipulator who succeeded once across 33 attempts (3\%). Even though they still exerted some influence on the MAAI by shifting the target principle upwards by 0.69 ranks in the final preference ranking, the disparity to the high intelligence manipulator is striking and suggests that the intelligence of LLMs is a key factor in influencing entire MAAI, presumably beyond the fairness domain that is the focus of this thesis.

The high success rate and substantial preference shift by seemingly simple modification of one Agent, by changing their personality prompt, indicates that normative judgments by AI agents remain fragile. Although all four principles possess normative appeal and none represents malicious intent, the degree to which the manipulator agents reshaped group preferences raises concerns about MAAI robustness in adversarial contexts. The vulnerability becomes more salient when considered alongside the expanding deployment of MAAI. These systems increasingly interact with diverse data sources and external agents through protocols such as the Agent-to-Agent protocol, creating numerous exposure points for potential manipulation \citep{Surapaneni_et_al_2025_A2A}.

In light of this, current practices that mostly insert normative goals through simple prompt-level interventions, such as instructing agents to ``be fair,'' appear insufficient \citep{Allmendinger_2025_MAAI_Fairness_Unpublished}. The findings point toward the necessity of embedding normative behavior at deeper architectural levels. Potential approaches include model fine-tuning on normative reasoning tasks or incorporating fairness considerations directly into model training objectives. The critical question becomes not whether AI agents can be influenced, but rather how to design MAAI systems that maintain normative consistency despite capability asymmetries and potential adversarial pressure.

More research is needed on how to effectively and robustly embed fairness into MAAI. The presented findings are the results of a replication of an experimental setup, which limits external validity to real world applications and to contexts where fairness is less explicit.

\subsection{Hypothesis 4}

Existing studies on fairness in MAAI rely exclusively on the English language in their system design \citep{Allmendinger_2025_MAAI_Fairness_Unpublished}. To address this gap, Hypothesis 4 explores the influence of input language on fairness outcomes. The three most used languages in the world are compared: English, Mandarin and Spanish. The null hypothesis that there is no significant difference is rejected.

This result is primarily driven by the difference from the Spanish to the Mandarin and English outcomes. The main difference is the high no consensus rate among the Spanish results. Whereas it is clear that the selection of the Spanish language is the root cause given that all other parameters are identical across languages, the causal mechanism remains unclear. Upon qualitative inspection of the result data, Spanish agents appear to have trouble delineating their identity in the group discussion from the one of other agents, thereby hindering them in successfully participating in the experiment. This could imply that the intelligence of the agents is affected by the language selection and significantly lower in Spanish. The large variance of values selected when consensus was maximizing the average with a floor constraint could be a sign of confusion when selecting the value and can be interpreted to support the low intelligence explanation.

Language affects the fairness judgments. Among the Spanish configurations, the principle maximizing the average with a range constraint is selected twice. This is, excluding the manipulative outcomes of Hypothesis 3, the only times this principle is selected across all MAAI runs for all hypotheses. Furthermore, among initial preferences, this principle is the top ranked principle 58 times for Spanish, whereas it is selected twice for Mandarin and zero times for English. This indicates that the fairness judgment is different when Spanish is used. This principle limits inequality through the range constraint while aiming to maximize overall prosperity, therefore it indicates a strong inequality aversion among Spanish agents. This could be rooted in different value judgments of Spanish speakers whose text data is presumably used to train the Agent's underlying LLM. Supporting this explanation is the predominance of left wing parties in Latin America where most Spanish speakers live throughout the 21st century. This could reflect a preference of Spanish speakers for left wing values, like limiting inequality \citep{Feierherd_et_al_2023_Pink_Tide_Inequality}.

In contrast, Mandarin agents prefer the maximize the floor principle more often compared to Spanish and English speaking agents. The latter selected this principle zero times, whereas the Mandarin MAAIs selected it once. Furthermore, the principle is the top ranked principle more often with 27 selections compared to 3 and 2 for English and Mandarin respectively at the initial preference ranking. This principle protects the worst off in a given society, which is in line with the Confucian value of ``Ren,'' which expresses benevolence to the worst off. In China, where Mandarin is predominantly spoken, Confucianism emphasizes group-based thinking compared to individualism and is widespread. This is reflected in Mandarin text and could through this channel influence the LLM into this thinking when prompted in Mandarin \citep{Chen_2024_Moral_Values_China_vs_USA}.

Meanwhile, a more individualistic view prevails in the English speaking world, especially in the United States of America \citep{Chen_2024_Moral_Values_China_vs_USA}. This could explain why English Agents preferred the maximize the average principle, as it emphasizes the individual compared to the group, in their initial preference ranking more often with 18 times compared to 2 and 3 times for Mandarin and Spanish agents respectively.

Among all languages, preferences shift throughout the experiment towards maximize the average with a floor constraint, which is the most popular principle chosen when consensus was reached and the most popular principle in the final preference ranking across all three languages. This indicates that the experiences in the individual phase of the experiment and the group discussion in the group phase influence the preferences and behavior of the agents. It also demonstrates that the initial preferences provide a good indication but are insufficient for predicting the final outcome. Moreover, taking the Mandarin agents as an example, merely predicting the outcome based on the initial preferences is insufficient, which supports the assumption behind this research that through persistent interactions between AI agents, their fairness preferences and behavior are influenced.
\iffalse


\subsection{Overarching Comparison}





\subsubsection{Technical}
Automated Translation 


Low Temperatue Same Names --> Agent confusion who they are for less smart modes

Different languages --> Less intillegent not differetn core beliefs

Few LLMs with only Gemini models

Low sample size 



Very rigid system --> experimental setup --> MAAI in practices probably more free
\fi


\subsection{Limitations}
\label{sec:Limitations}

As outlined in \Cref{sec:Fairness in AI}, fairness is a multidimensional concept that requires the fulfillment of multiple desiderata according to the framework by \citet{Deck_2024_MappingPotential}. This thesis focuses on the desideratum of Formal Fairness and does not analyze MAAI through all desiderata, such as perceived fairness. Therefore, when making statements about fairness in the context of this thesis, they relate primarily to Formal Fairness. This decision reflects the limited scope of this thesis. However, future research should broaden the scope and, for example, adapt the artifact presented to enable humans to participate in the experiment alongside AI agents to gain insights into perceived fairness in MAAI systems. 

The research design of this study replicates the work by \citet{Frohlich_Oppenheimer_1992_Book} by replacing human subjects with AI-based agents. This results in a rigid environment where agent actions are constrained. In the individual phase, agents return preferences, make choices on distributions, and manage their memory. In the group phase, this action set extends to formulating strategies, making statements to other agents, and voting. The predetermined sequence of actions decreases agent autonomy. In contrast, a high degree of autonomy is a key differentiator of the MAAI paradigm from robotic process automation \citep{Allmendinger_2025_MAAI}. However, autonomy is not a binary concept but a spectrum. The low degree of autonomy in this study results from the replication design. It ensures comparability to the human baseline study by \citet{Frohlich_Oppenheimer_1992_Book} and decreases the action space for errors, thereby enabling the use of less capable models that remain relevant in applications using edge computing. For example, during development, the Gemini 3 1b model successfully completed the experiment with an Artificial Analysis Intelligence Score of 7, which is significantly lower than the average score of 49.7 of the LLMs used in testing Hypothesis 1 \citep{ArtificialAnalysis_AI_2025}. Increasing the autonomy and tool set available to agents should be investigated further to understand fairness outcomes in this context better. The artifact developed in this thesis can serve as a foundation because tools can be integrated through the framework, including Model Context Protocol-based tools, which are becoming an industry standard \citep{Ray_2025_SurveyMCP}. 

On a related note, this thesis implicitly assumes that AI agents behave in industry applications where fairness is an implicit aspect of decisions, such as when hiring suggestions are made by an MAAI system, whereas in the presented research, AI agents explicitly operate in a fairness context and are aware of that. With human subjects, it is well documented that their behavior outside of laboratory contexts differs from their behavior within such contexts \citep{Mitchell_2012_External_Validity}. Whether this holds for AI agents is unknown as of writing this thesis and presents an opportunity for further research. However, leading LLM providers like Anthropic are working to improve the consistency of LLM personas that behave consistently across different contexts, which increases the external validity of this approach \citep{Anthtropic_Chen_2025_Persona}.

The variation of configurations tested in this study is limited. For example, the personality prompt, with the exception of the manipulator prompt, merely states to behave like a college student or an American college student respectively. This could be varied to better understand how personality and instructions affect fairness outcomes. For instance, AI agents could be instructed to behave fairly or unfairly. However, this thesis had to focus on a limited set of questions to not exceed the scope and because of cost constraints. The \$300 Google Cloud balance was completely utilized in running all configurations. Therefore, future research can build on this artifact and vary hyperparameters to better understand their influence. Furthermore, because of cost constraints, models beyond Google's offering could only be tested in a very limited scope in Hypothesis 2, which presents a related limitation. 

The findings regarding Hypothesis 4 are limited by the fact that the author of this thesis and the corresponding artifact neither speaks Spanish nor Mandarin and therefore resorted to machine translation for the creation and analysis of these versions. Because machine translations can be imperfect, this reliance could introduce errors and misinformation that could affect the outcome of this hypothesis. However, there is no indication that this is the case.



% Explain 
% Justify
% Furher research

%Desing of Study pushes participant towards max. average with floor constraint
%--> Critique Baseline Epxeriment Design
%--> Uncharitable Rawls (Ãœberleitung des Himmels) 
%--> Higher Intelligence of LLMs copared to Humans??? 

%Hypothesis one by one, then all together 




 
